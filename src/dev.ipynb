{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RRDBNet' from 'SRGAN_model' (c:\\SaiVinay\\Thesis\\Code\\dev_03_02\\SRGAN_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSRGAN_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RRDBNet\n\u001b[0;32m      3\u001b[0m G \u001b[38;5;241m=\u001b[39m RRDBNet(in_nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, nf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, nb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, gc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      5\u001b[0m G\u001b[38;5;241m.\u001b[39mparameters\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RRDBNet' from 'SRGAN_model' (c:\\SaiVinay\\Thesis\\Code\\dev_03_02\\SRGAN_model.py)"
     ]
    }
   ],
   "source": [
    "from SRGAN_model import RRDBNet\n",
    "\n",
    "G = RRDBNet(in_nc=3, out_nc=3, nf=64, nb=32, gc=32)\n",
    "\n",
    "G.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yml file\n",
    "import yaml\n",
    "with open(\"C:\\SaiVinay\\Thesis\\Code\\dev_03_02\\opt.yml\") as f:\n",
    "    opt = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import define_D, define_G\n",
    "\n",
    "netG = define_G(opt)\n",
    "netD = define_D(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_imgs = os.listdir(lr_dir)\n",
    "        self.hr_imgs = os.listdir(hr_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.lr_imgs), len(self.hr_imgs))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img_name = os.path.join(self.lr_dir, self.lr_imgs[idx])\n",
    "        hr_img_name = os.path.join(self.hr_dir, self.hr_imgs[idx])\n",
    "        \n",
    "        lr_img = Image.open(lr_img_name).convert(\"RGB\")\n",
    "        hr_img = Image.open(hr_img_name).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            lr_img = self.transform(lr_img)\n",
    "            hr_img = self.transform(hr_img)\n",
    "        \n",
    "        return lr_img, hr_img\n",
    "\n",
    "def get_loader(lr_dir, hr_dir, batch_size, num_workers=4, shuffle=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # You can add more transformations here such as normalization, resizing, etc.\n",
    "    ])\n",
    "    \n",
    "    dataset = SRDataset(lr_dir, hr_dir, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    \n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdw0lEQVR4nO2da4xd13Xf/+u+5s6TwxlyyBEpibJMyZIdibbGjAIHqRs3hmoEsP3BRowgEBAjDIoYqIvkg+ACtfvNLWoH/lAYoGMhSuE6Nio7VlOjtaImUFLEkimboijT1sskRXJKcvia133f1Q9zVVDq/q8ZcWbusN7/HzCYe8+6+5x19jnrnHv3/6y1zd0hhPjlp7DVDggh+oOCXYhMULALkQkKdiEyQcEuRCYo2IXIhNJ6GpvZQwC+AqAI4M/c/YvR54cHqz4+Npq0FYxLgKVC2lYK2kSCYrfLrZ2gHYhMWQoumQUz7scNq568Id1a0FdRZ7lz/xHsmxFPHN0b8iPYs9BmZKUW+B7tV8SNteK77TewxgtXFzG/VE82vOFgN7MigP8I4LcAnAHwIzN7wt1/ytqMj43iX/zux5K2YWvTbU0OtZLLJwZ4aHaCsF2s8xNuoc1tBU/btlf5WVqt8C5utoKDGTz/0HW+b8VCuh8LZDkAWLdIbe02t7lVqK1UTO9bp5M+lgAQdD3MeD9acEEqkotLuVSmbQpFvq1ucMjKFlz1g3ZtEu7tTnQXSR+XP/nqE7wJX9uqHATwiru/5u5NAH8J4KPrWJ8QYhNZT7DvAfD6de/P9JYJIW5C1hPsqS8m/8/3ETM7ZGZHzOzIUq2+js0JIdbDeoL9DIBbr3u/F8C5t37I3Q+7+4y7zwwPVtexOSHEelhPsP8IwH4zu8PMKgB+BwAfHRBCbCk3PBrv7m0z+wyA/4EV6e1Rd38xamMAymTksdPho8+Xl9JDmfUGH+KslPgocjcYva0Yb1ck8pVFUl4wmt3p8u5vBBpgAXxkvdBKNywU+VB3tcD7I5J/2h1ua7bSPjY7Ddqm0+U+DpT4yP9AKRipJ6oAgj4sBrfAUrCt6N7ZavF9a7WJglIMzuFAQWGsS2d39+8D+P561iGE6A96gk6ITFCwC5EJCnYhMkHBLkQmKNiFyIR1jca/XYrWxfZy+im6TiBflZh6EiQedMGliVIhkKHKQVIFcdEj6c25j+Zc/uk2m9RWq9X49oiEOTA4wP0YDh52ihI4goScJuniSFIkeUYrtiATLcqkKxNTK0gM8iK3VZ33Y3DKIbqvFogUHN2JjfrPz0Xd2YXIBAW7EJmgYBciExTsQmSCgl2ITOjraLwZUCYjlhU21A2gXEhfk9i6AKAYJDqUglH8QpDoUKCllvhocJR0UyC19QBgMEh06AblirpNsj2PyktxH8tBMkaEe3qEv1jmCS3dDh+qLwTnR4GcHxGVYASfjY4DgEX3xy63sTqKAODldLsCKT0FAE78YLX/AN3ZhcgGBbsQmaBgFyITFOxCZIKCXYhMULALkQl9lt4MJTITx2CJSxNFIpV5MKVRJZhhBpG0EihNrB5bJ5LyostpINVYIFFVizxxpcgktkBea0eyUDhVVtCPxGRFLq91g/V1u0HdveAsrhLpsFDgM8JEt8BOWPstqtfHW7VJshSbgQhYSSpLo0QYIbJHwS5EJijYhcgEBbsQmaBgFyITFOxCZMK6pDczOwlgAUAHQNvdZ1ZpASfalgXyD4jE1u1yPWOhyW1suh0AqFS4JMOy7xDIKo1AuvJguqN2IA86lV1AL9+taKqmdiTXcFs32DeQY1MJjnOFZH8BgHuQjci9QJO0W27xNoU279+K8YMdlOtDO8hUbBJZsR1kUw6QLMCwliO1rJ1/6u5zG7AeIcQmoq/xQmTCeoPdAfzAzJ4zs0Mb4ZAQYnNY79f4D7j7OTObAvCkmf3M3Z++/gO9i8AhAJjcNrzOzQkhbpR13dnd/Vzv/wUA3wVwMPGZw+4+4+4zo0OD69mcEGId3HCwm9mwmY2+8RrAhwEc3yjHhBAby3q+xu8C8F1bSW8qAfjP7v7fowYOQ6OblrY6QZIaiEQVKBNotrgEEdkGAolkiFS4DCWXQMYJVC14kElngfTmnfS0Uc1u5CVnIKqvGGQddokeGcy8hUpQQdQ8kvl4uwY5nmSWLABAOdivdtCNkezVDPxvkJU2g3NxicjH0el2w8Hu7q8BuP9G2wsh+oukNyEyQcEuRCYo2IXIBAW7EJmgYBciE/pacLLrQIMURGwGmgYrROgtLrm0AtmiFugu9UADXKynbZWgqmS5HBQ2DKpbeiCVBaoR3NM+Rm0K4D42gjnziqH0lt63elBkczFIRbOg+GKpwH004mMoRAbSZrsQnKfBfHrRnH9srr1iJHvS80NzvQmRPQp2ITJBwS5EJijYhcgEBbsQmdDX0XgANKnl4vwybXKtmbaNt67QNvt38hHm7jC3jVWCul+ltG15oca3FYzQLjT4yGmtxmudFSrBYaukawbUfIhvKxirLxe4/+VgiL9EmrXri7RNq55O4gGASpVPh2VB0pB7uh89OPWbgS0q/xfl6kTKC5tGqxsoEE5sseoihMgCBbsQmaBgFyITFOxCZIKCXYhMULALkQl9ld6W6g0889IvkrZLyw3abt/EQHL5djtP24w3eSXb0YH0+gBgfoknY1SG0pJdeZB3oxe5bTyQjC5d4TLUO6e4ZNfuXksuX2jM0zY//Oklajt7jUuAk1OT1DZaIjXommn/AKA4uIvaSoNcegtmAaO194rBcQkTULiJTlMGAIUgQcVJclA3cKRL79NKhBEiexTsQmSCgl2ITFCwC5EJCnYhMkHBLkQmrCq9mdmjAH4bwAV3f09v2QSAbwHYB+AkgE+6O09B69HudnF5MZ3BNjJcpe3mltMy1E8u80y54vxVapvcxre10OLSxZmFtLRSqnIpb/v4KLX9yv7t1PbAvVzWsvoStcHT0uGenVyKvH1qN7X98Ohpanv++E+orV1K9+O+26ZomxbvRiw0eJ25+iLvj3ozrcuNTUzQNpUyd4TVQwQQSm/laN4rkkpnwb24wGrhBe6t5c7+5wAeesuyRwA85e77ATzVey+EuIlZNdh7861ffsvijwJ4rPf6MQAf21i3hBAbzY3+Zt/l7rMA0PvPv5sJIW4KNv1xWTM7BOAQAFSD37ZCiM3lRu/s581sGgB6/y+wD7r7YXefcfeZSiWYMEEIsancaLA/AeDh3uuHAXxvY9wRQmwWa5HevgnggwB2mNkZAJ8H8EUA3zazTwM4DeATa9mYd9poXktnWM1e5NedUictvZWWF2ib+6q8wN/VeV4g8hdXuXwy20ivs1zlvh+d4wUWtw/zdK0TL/Hsu/ExngE2WkhnD3qLfvnC6DD/efX+e6ap7dfu41lqL594Pbn8pVM8661Zn6W2cw0uHS7V+fGsIN3HpQLv+4VSumgnABQHAlt1hNuCapQFJtkFU2W1WZOoCCg3reDunyKmD63WVghx86An6ITIBAW7EJmgYBciExTsQmSCgl2ITOhrwUnvOhqNtKTk4A/cGJnX6r27uWRkxqWra3WeGlR3fv2zUtrH1xd5RlO3yP04fpJncl1erFPblWvcNllKb++OMb5f1S7349kO76v7Duyltne/5/a0wU/RNs+/OkdtF69wKbUQlIEcqqa1qKvneMZks8KzEQdGtlFbKcgs9GJwfpP5D9k8dQDQIRJbO0iu051diExQsAuRCQp2ITJBwS5EJijYhcgEBbsQmdBX6a3TdVxdTstGw0M8XWdmezqT68DeIdrmxQtctrhW59e4pSBrqEYUqkIwb1ityedsO3eJ23YOcx87ZB41AHjf3TuTy8dLXJM5dY4XbOw6l/me+JufB+3Sy++65x20zckrL1Nb6zz38f17gyzAStqR505dpW1K/LTCIK9Vik6X99VyEGpOdLQgUe6G2ujOLkQmKNiFyAQFuxCZoGAXIhMU7EJkQl9H40tFw47R9Mjp/h08qeX+HWk3Ty/wJI2a8V1rFPlo9liBD2eOkJHdC8s8kWSgwkfBZ6b5tfa5l/lsWoMDPKni1devJpdv38aTNB64i09RdfUi93/uGk9O+bsfpqeN2rWTJ5nsv43XtCsVeH9MDPJj1mqnE4NGSrzvJ4bS6g8ADA3wEffLLa4YLAWqzJVGuo9bHd735VK67ztdfm7rzi5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhMWMv0T48C+G0AF9z9Pb1lXwDwBwAu9j72OXf//mrrqhQMt46kZaP9w1yasHI6M6E0wKW3qTK3+SKXVrYHSSbbSTLJlXnu++07eJLGr9zGu3/38Bi1Pfq/rlLbyy+n5Z9OiU+7NHOSZ3fsGQiKmoFLb9vJ9ErlApcpx3ZwCXB/iR9PtPn0T3WSvfT+O3ltwItXuW15gU/n1Vr+BbVV2sF0U610Xbu68+MyVE3HUaezPuntzwE8lFj+p+5+oPe3aqALIbaWVYPd3Z8GcLkPvgghNpH1/Gb/jJkdM7NHzYw/FiWEuCm40WD/KoA7ARwAMAvgS+yDZnbIzI6Y2ZF6i/9eE0JsLjcU7O5+3t077t4F8DUAB4PPHnb3GXefqZb7+ii+EOI6bijYzWz6urcfB3B8Y9wRQmwWa5HevgnggwB2mNkZAJ8H8EEzOwDAAZwE8Idr2djYYAEfvicto1mQrXNyPn1Nem2Bb2ukwq9jo4N8t8eCbKh33ZL2vVLh0s/0Dp6hdst2/rPG2rzdr+7jksyr19LTGs3Oc7nxmQb345ZhLq99aB/PVHzg3ePJ5RNTPPvutdM8a6zg3MfJSe5HrZ6WPrcN8f3q+iVqm7/Aj3Wxzfu40+bn1RiRFSeGaRO0iulMv6LxDMBVg93dP5VY/PXV2gkhbi70BJ0QmaBgFyITFOxCZIKCXYhMULALkQl9fcqlXAB2D6dlhkVwneH46XRW2eUFXvyvXOTXsYPTfLdZUUkAGB9Ly2EPjvPMtiGuNGHbKM8oqzmX3uadZ2VVi+k+mZniPt5FCnoCwM4B3o97dvCde+fdu5PLiwNcNtyxg/f9wmWebVYO5NJWKd1XlRHeH+/av4Pa2oEsN3c5LXsCQD1IHhxupzXkUjuQBy0t8xVcBSeFyB4FuxCZoGAXIhMU7EJkgoJdiExQsAuRCf1NMPcurJmWhpbAM5fK5bQMVSKyCgDUOlzGubzAC0S2q7yw4cJyWgrZf2s6Gw4A0OXZWkdf4lLNUIm3O7CHX6Pnzqall1+/e4S2OX+N9+OB2/m+3baPS1QDY+kiios1Lg1t28bPAW/yvjJwXWt4MH3Mlhb4PneCQpq7d4xTW8X4cRmq8Yy4uaX0sT6zyKvB7ZkkGZjG+0J3diEyQcEuRCYo2IXIBAW7EJmgYBciE/o6Gm+FIkqj6Sl+Lp7n7bydHq008BH3SoGPSs63+DVuqcATJH5yLu1Hq8FHTT0YhV28xuuZ/dr9PMlkepDv977hdAn/2/fx0fim89OgXuAJSm3no+fLtfRo9+ROvl/dNh8hHx3hCTSNGh+pHxwiPhb4Pl86yafKWm7xvp/YPUlt4zVeXw/n09u7ssD3a7KUVgyKGo0XQijYhcgEBbsQmaBgFyITFOxCZIKCXYhMWMv0T7cC+AsAuwF0ARx296+Y2QSAbwHYh5UpoD7p7leiddU7wCukhNdLc1x2qXs6OcU7PFnEuUKC8w2e7FKsc6msRJJrdjuXVW4b5T6+Yz+XrqamueTVRVq+BIDdd6elwzKRPAHgwhWenDJ7isuD1SpPGBkfTPtRWwoSUBpcavJW0C6QDlvt9DGzCve93uK2YpmfWFWSdAMAxcF0YhAATBO1rBnIfN4h/RGc+Gu5s7cB/LG73wPgQQB/ZGb3AngEwFPuvh/AU733QoiblFWD3d1n3f3HvdcLAE4A2APgowAe633sMQAf2yQfhRAbwNv6zW5m+wC8F8AzAHa5+yywckEAMLXh3gkhNow1B7uZjQB4HMBn3X3+bbQ7ZGZHzOzIInmEUgix+awp2M2sjJVA/4a7f6e3+LyZTffs0wAupNq6+2F3n3H3mZFBPvGBEGJzWTXYzcywMh/7CXf/8nWmJwA83Hv9MIDvbbx7QoiNYi1Zbx8A8HsAXjCzo71lnwPwRQDfNrNPAzgN4BOrrajWdBw9l/4qvxDUJjNLSxqVKq+P1iSSCwAYuBzWbXHpbbyYluzKwXRMp/43l65uuW2c2orDXHorlvg3pMpoWuJZCOru/ZzpoQDOzfEsqvvfxTPY5pfS21u8xv24ZYKaYMbPj8V5vs75y2n/Bwb5qT82xqXZ0gCXS5eW+XkwNMSP2fhkWhYtV3gGZrOV3udKmavfqwa7u/8DALb3H1qtvRDi5kBP0AmRCQp2ITJBwS5EJijYhcgEBbsQmdDXgpPFAjAxmB7YX2oG0y410/JJl0hyANAkRSoBYGSIFy9sdrnEU2umZbmacelnyPh+VStcjqlOjFFbs8HlsBJZ55U5Lskc+xl/IHLPFJcA5y7zLLVT59KS4x1TQbZZjffHIDlvAKAYnMVXFtLnQdeDKaOCjLjFJj8/yoEkWihwKXiompbYDHx9JUvLfCVSiBLQnV2IbFCwC5EJCnYhMkHBLkQmKNiFyAQFuxCZ0Oe53gook6yhDlcMMELmNptb5NJPi8hkANCOJK8S75LFZlpOGp/iUt5kmUs8k7t5EULvcqmmZPwa3VhMF7/8x6MXaZuzl+rUVizwA/PaLM/ou2dvOiNux0hwynV51tjSPO+PYplnh41vS0t2tQaX0JrO+7da5hJg0FUoVYL9bqfXORTIjY0621jgH/dACPHLhIJdiExQsAuRCQp2ITJBwS5EJvR1NL4AxyBJCKgYHz2vkdFK7/AR1SDPAd0WT5IpFfiI8DAZ9N2ze4S22TMZ1MILRnbR5ErD0gLf78efnE0u/9tjC7TNUJD4MRuM1N+1h9cA3MW6hNROA4BiMGJdCioT10miFACUy+l9a/BTAM1ACYkG1Qse1Mlb5D6apVdaDJKoisX0fTpooju7ELmgYBciExTsQmSCgl2ITFCwC5EJCnYhMmFV6c3MbgXwFwB2A+gCOOzuXzGzLwD4AwBvZFh8zt2/H62rUnDcPpKWcgaIlAAAr1xOyyftLpdj5lpcQqsvc1mrWOTyyd6p9PY6QeJEpxNIb12u/yyT2mkA8GePn6K2//pcup5chUhQAHD3FD8Ndg9xLWeizH0cH00nBw0UeX+0wG31RpAks8xlreXltI/DI3y/gnwcWJBE1Q3OAwS1DZ3Uk6t3uI/lgfTx5D24Np29DeCP3f3HZjYK4Dkze7Jn+1N3/w9rWIcQYotZy1xvswBme68XzOwEgD2b7ZgQYmN5W7/ZzWwfgPcCeKa36DNmdszMHjWz7RvtnBBi41hzsJvZCIDHAXzW3ecBfBXAnQAOYOXO/yXS7pCZHTGzI/O14BlFIcSmsqZgN7MyVgL9G+7+HQBw9/Pu3nH3LoCvATiYauvuh919xt1nxoI5sYUQm8uqwW5mBuDrAE64+5evWz593cc+DuD4xrsnhNgo1nKr/QCA3wPwgpkd7S37HIBPmdkBrIz2nwTwh6utaLhqOHh3epP/7RmelXVpLi1btMDlpFad10cLZobC+DZez2xpKZ2xVQukn+4o39bcWT7t0vwSl2qOneKZYyNDaf93D/Pr+p0T3HbLGN+3saGgFl4zLScZ8Q8ASmVey68a3Je6oVaWblev8TZmQb07viUs1/hx6bT5Ogtl4mMzkG0LRJbzoHYhtfzftv4PSFexCzV1IcTNhZ6gEyITFOxCZIKCXYhMULALkQkKdiEyoc/TP/FsnepAIHnVrySXLwSKSzvINnvnTl4ocVuJF768fTLt+2CVZyctBxLasy+ki0MCwLOvcRlnosoP285q+vo9EmXzbefrmxzjYtNAkEkHkrHVDKZdqgRTZaHD9dJ2MNWXkVWWgmKf3uW2pSXuR73N961e5+sc6aadbC7z9Q2Q+3SgvOnOLkQuKNiFyAQFuxCZoGAXIhMU7EJkgoJdiEzoq/TWbjsuXU1LSqcucfnkPNHYOsHEVvft4elmD+zhRQN3VLj8U2+lbS+d4QUsLZg37PjrXMb5+1cXqW3XKD9stwynl08NcZnMgonxGi3ex40OzywcGko7MhTIhu0GPwe4CAVYhd+zmvX0fncCjcqDtMh6m/fVhSt8XryxQS4tF4bS52Ozyff6YjO9rXbgn+7sQmSCgl2ITFCwC5EJCnYhMkHBLkQmKNiFyIS+Sm/Xltv462cvJ21Pn+JyR7eYdnPvrgna5o5bx6nt3buuUdvZy0vU9oMX0+32jnEp7wyRGgHg5BUurRy4e5rafuvBu6ht7uLV5PLmudO0TbfFJa8S+L4tcaUJ5y+mly/X+XEeHOHy1GzQV+1gjrWBgfTyUoG3KZW4TFlIlmNcoVzi/luRr3NhMd0njVZQPLLM+iMobEktQohfKhTsQmSCgl2ITFCwC5EJCnYhMmHV0XgzqwJ4GsBA7/P/xd0/b2YTAL4FYB9Wpn/6pLuni8X1aHkFZ7u3Jm3vvmeEtitYeiRz5zhvM1VOj/oDwMmrl6jtr47wBJQzl9IjoOcbfMT6p2eD+mhkaiIA+Fe//6vU9k8O7qe2VjM9+t9ZOkPbnPj7Z6ntapDcgTLf7+V2etT69TN8fW68qODI0CC1bRvhI90louR0gmSXcoWPuC9ENejIlFcAMFQJklqW0skr4wN8ZH25nl7femvQNQD8prvfj5XpmR8yswcBPALgKXffD+Cp3nshxE3KqsHuK7xxuyv3/hzARwE81lv+GICPbYaDQoiNYa3zsxd7M7heAPCkuz8DYJe7zwJA7//UpnkphFg3awp2d++4+wEAewEcNLP3rHUDZnbIzI6Y2ZFaIyj0LoTYVN7WaLy7XwXwdwAeAnDezKYBoPf/Amlz2N1n3H1mcIAP6AghNpdVg93MdprZeO/1IIB/BuBnAJ4A8HDvYw8D+N4m+SiE2ADWkggzDeAxMyti5eLwbXf/azP7RwDfNrNPAzgN4BOrrWhooIIH7tybtBXZPD0ALpJSZ4M0GQBYaPMpnp45XaW2C8vcjzZJZrhQ437M1/hPl3v3TlLb1EiQBMHzLTAymc78KJXT/Q4A7Stnqe1//s1L1DY3x/d7ZFtaKutWeN8PlvnpGNWgC2b6wnItLUUOB7XwFoNj1iZTNQFAg6ty8AL/Vuuklp8H0myDbKwbaG+rBru7HwPw3sTySwA+tFp7IcTNgZ6gEyITFOxCZIKCXYhMULALkQkKdiEywTxKk9nojZldBHCq93YHgLm+bZwjP96M/Hgz/7/5cbu770wZ+hrsb9qw2RF3n9mSjcsP+ZGhH/oaL0QmKNiFyIStDPbDW7jt65Efb0Z+vJlfGj+27De7EKK/6Gu8EJmwJcFuZg+Z2c/N7BUz27LadWZ20sxeMLOjZnakj9t91MwumNnx65ZNmNmTZvZy7//2LfLjC2Z2ttcnR83sI33w41Yz+1szO2FmL5rZv+wt72ufBH70tU/MrGpmz5rZ8z0//m1v+fr6w937+gegCOBVAO8AUAHwPIB7++1Hz5eTAHZswXZ/A8D7ABy/btm/B/BI7/UjAP7dFvnxBQB/0uf+mAbwvt7rUQAvAbi3330S+NHXPgFgAEZ6r8sAngHw4Hr7Yyvu7AcBvOLur7l7E8BfYqV4ZTa4+9MA3lrruu8FPIkffcfdZ939x73XCwBOANiDPvdJ4Edf8RU2vMjrVgT7HgCvX/f+DLagQ3s4gB+Y2XNmdmiLfHiDm6mA52fM7Fjva/6m/5y4HjPbh5X6CVta1PQtfgB97pPNKPK6FcGeqsC/VZLAB9z9fQD+OYA/MrPf2CI/bia+CuBOrMwRMAvgS/3asJmNAHgcwGfdfb5f212DH33vE19HkVfGVgT7GQDXTwuzF8C5LfAD7n6u9/8CgO9i5SfGVrGmAp6bjbuf751oXQBfQ5/6xMzKWAmwb7j7d3qL+94nKT+2qk96276Kt1nklbEVwf4jAPvN7A4zqwD4HawUr+wrZjZsZqNvvAbwYQDH41abyk1RwPONk6nHx9GHPjEzA/B1ACfc/cvXmfraJ8yPfvfJphV57dcI41tGGz+ClZHOVwH86y3y4R1YUQKeB/BiP/0A8E2sfB1sYeWbzqcBTGJlGq2Xe/8ntsiP/wTgBQDHeifXdB/8+HWs/JQ7BuBo7+8j/e6TwI++9gmA+wD8pLe94wD+TW/5uvpDT9AJkQl6gk6ITFCwC5EJCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhMU7EJkwv8BAy8lp3dFisYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_img_name = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K\\valid\\LR\\0901.png\"\n",
    "lr_img = Image.open(lr_img_name).convert(\"RGB\")\n",
    "pre_process_lr = transforms.Compose([transforms.RandomCrop(32),\n",
    "                                               transforms.ToTensor()])\n",
    "lr_img = pre_process_lr(lr_img)\n",
    "lr_img = lr_img.permute(1, 2, 0)\n",
    "print(lr_img.shape)\n",
    "plt.imshow(lr_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, size=32, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.size = size\n",
    "        self.lr_imgs = os.listdir(lr_dir)\n",
    "        self.hr_imgs = os.listdir(hr_dir)\n",
    "        self.pre_process_lr = transforms.Compose([transforms.CenterCrop(self.size),\n",
    "                                               transforms.ToTensor()])\n",
    "        self.pre_process_hr = transforms.Compose([transforms.CenterCrop(self.size*4),\n",
    "                                               transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "            return min(len(self.lr_dir), len(self.hr_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(self.lr_dir) != None and type(self.hr_dir) != None:\n",
    "            print(idx)\n",
    "            print(self.lr_imgs[idx], self.hr_imgs[idx])\n",
    "            lr_img_name = os.path.join(self.lr_dir, self.lr_imgs[idx])\n",
    "            hr_img_name = os.path.join(self.hr_dir, self.hr_imgs[idx])\n",
    "            lr_img = Image.open(lr_img_name).convert(\"RGB\")\n",
    "            hr_img = Image.open(hr_img_name).convert(\"RGB\")\n",
    "            if min(lr_img.size) >= self.size and min(hr_img.size) >= self.size*4:\n",
    "                lr_image = self.pre_process_lr(lr_img)\n",
    "                hr_image = self.pre_process_hr(hr_img)\n",
    "                # show_lr_img = lr_image.permute(1, 2, 0)\n",
    "                # show_hr_img = hr_image.permute(1, 2, 0)\n",
    "                # fig, axis = plt.subplots(1,2)\n",
    "                # axis[0].imshow(show_lr_img)\n",
    "                # axis[0].set_label(\"LR Image\")\n",
    "\n",
    "                # axis[1].imshow(show_hr_img)\n",
    "                # axis[1].set_label(\"HR Image\")\n",
    "                \n",
    "                # plt.show()\n",
    "                \n",
    "                return hr_image, lr_image\n",
    "        \n",
    "lr_dir = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K\\valid\\LR\"\n",
    "hr_dir = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K\\valid\\HR\"\n",
    "\n",
    "dataset = CustomImageDataset(lr_dir, hr_dir)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0901.png 0901.png\n",
      "1\n",
      "0902.png 0902.png\n",
      "2\n",
      "0903.png 0903.png\n",
      "3\n",
      "0904.png 0904.png\n",
      "4\n",
      "0905.png 0905.png\n",
      "5\n",
      "0906.png 0906.png\n",
      "6\n",
      "0907.png 0907.png\n",
      "7\n",
      "0908.png 0908.png\n",
      "8\n",
      "0909.png 0909.png\n",
      "9\n",
      "0910.png 0910.png\n",
      "10\n",
      "0911.png 0911.png\n",
      "11\n",
      "0912.png 0912.png\n",
      "12\n",
      "0913.png 0913.png\n",
      "13\n",
      "0914.png 0914.png\n",
      "14\n",
      "0915.png 0915.png\n",
      "15\n",
      "0916.png 0916.png\n",
      "16\n",
      "0917.png 0917.png\n",
      "17\n",
      "0918.png 0918.png\n",
      "18\n",
      "0919.png 0919.png\n",
      "19\n",
      "0920.png 0920.png\n",
      "20\n",
      "0921.png 0921.png\n",
      "21\n",
      "0922.png 0922.png\n",
      "22\n",
      "0923.png 0923.png\n",
      "23\n",
      "0924.png 0924.png\n",
      "24\n",
      "0925.png 0925.png\n",
      "25\n",
      "0926.png 0926.png\n",
      "26\n",
      "0927.png 0927.png\n",
      "27\n",
      "0928.png 0928.png\n",
      "28\n",
      "0929.png 0929.png\n",
      "29\n",
      "0930.png 0930.png\n",
      "30\n",
      "0931.png 0931.png\n",
      "31\n",
      "0932.png 0932.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [165]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (hr_img, lr_img) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hr_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m lr_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoneType found in batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for batch_idx, (hr_img, lr_img) in enumerate(train_dataloader):\n",
    "    if hr_img is None or lr_img is None:\n",
    "        print(f\"NoneType found in batch: {batch_idx}\")\n",
    "        # Optionally, print or log other information such as file paths, batch indices, etc.\n",
    "    else:\n",
    "        pass\n",
    "        # Your existing data processing code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0936.png 0936.png\n",
      "0926.png 0926.png\n",
      "0918.png 0918.png\n",
      "0929.png 0929.png\n",
      "0944.png 0944.png\n",
      "0928.png 0928.png\n",
      "0917.png 0917.png\n",
      "0921.png 0921.png\n",
      "0940.png 0940.png\n",
      "0939.png 0939.png\n",
      "0908.png 0908.png\n",
      "0909.png 0909.png\n",
      "0912.png 0912.png\n",
      "0906.png 0906.png\n",
      "0930.png 0930.png\n",
      "0924.png 0924.png\n",
      "0907.png 0907.png\n",
      "0931.png 0931.png\n",
      "0941.png 0941.png\n",
      "0938.png 0938.png\n",
      "0920.png 0920.png\n",
      "0901.png 0901.png\n",
      "0902.png 0902.png\n",
      "0911.png 0911.png\n",
      "0932.png 0932.png\n",
      "0922.png 0922.png\n",
      "0925.png 0925.png\n",
      "0946.png 0946.png\n",
      "0913.png 0913.png\n",
      "0919.png 0919.png\n",
      "0914.png 0914.png\n",
      "0934.png 0934.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [121]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hr_img, lr_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "hr_img, lr_img = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0942.png 0942.png\n",
      "0932.png 0932.png\n",
      "0921.png 0921.png\n",
      "0933.png 0933.png\n",
      "0929.png 0929.png\n",
      "0938.png 0938.png\n",
      "0925.png 0925.png\n",
      "0935.png 0935.png\n",
      "0911.png 0911.png\n",
      "0915.png 0915.png\n",
      "0939.png 0939.png\n",
      "0940.png 0940.png\n",
      "0912.png 0912.png\n",
      "0927.png 0927.png\n",
      "0904.png 0904.png\n",
      "0905.png 0905.png\n",
      "0946.png 0946.png\n",
      "0910.png 0910.png\n",
      "0913.png 0913.png\n",
      "0926.png 0926.png\n",
      "0906.png 0906.png\n",
      "0902.png 0902.png\n",
      "0907.png 0907.png\n",
      "0918.png 0918.png\n",
      "0934.png 0934.png\n",
      "0924.png 0924.png\n",
      "0936.png 0936.png\n",
      "0944.png 0944.png\n",
      "0919.png 0919.png\n",
      "0914.png 0914.png\n",
      "0916.png 0916.png\n",
      "0941.png 0941.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display image and label.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m hr_img, lr_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR Feature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhr_img\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR Feature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_img\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melem_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:137\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    135\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(batch)\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "import matplotlib.pyplot as plt\n",
    "hr_img, lr_img = next(iter(train_dataloader))\n",
    "print(f\"HR Feature batch shape: {hr_img.size()}\")\n",
    "print(f\"LR Feature batch shape: {lr_img.size()}\")\n",
    "\n",
    "show_lr_img = lr_img.permute(1, 2, 0)\n",
    "show_hr_img = hr_img.permute(1, 2, 0)\n",
    "fig, axis = plt.subplots(1,2)\n",
    "axis[0].imshow(show_lr_img)\n",
    "axis[0].set_label(\"LR Image\")\n",
    "\n",
    "axis[1].imshow(show_hr_img)\n",
    "axis[1].set_label(\"HR Image\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dir = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K\\valid\\LR\"\n",
    "hr_dir = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K\\valid\\HR\"\n",
    "batch_size = 8\n",
    "train_loader = get_loader(lr_dir, hr_dir, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # You can add more transformations here such as normalization, resizing, etc.\n",
    "    ])\n",
    "    \n",
    "dataset = SRDataset(lr_dir, hr_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 9996, 11932, 24524, 12536) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display image and label.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_features\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 9996, 11932, 24524, 12536) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "import matplotlib.pyplot as plt\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SRDataset at 0x1c601c08910>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m(\u001b[38;5;241m9\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "train_loader.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SRDataset at 0x1c617797610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "Generator = netG\n",
    "Discriminator = netD\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "# Assuming you have datasets and models defined\n",
    "\n",
    "# Define the generator and discriminator networks\n",
    "generator = Generator  # Input size: 32x32x3, Output size: 128x128x3\n",
    "discriminator = Discriminator # Input size: 128x128x3, Output size: 1\n",
    "\n",
    "# Define loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "content_loss = nn.MSELoss()\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epoch:\"):\n",
    "    for batch_idx, (lr_imgs, hr_imgs) in tqdm(enumerate(train_loader), desc=\"Batches:\"):\n",
    "        print(len(lr_imgs), len(hr_imgs))\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(lr_imgs.size(0), 1)\n",
    "        fake = torch.zeros(lr_imgs.size(0), 1)\n",
    "        \n",
    "        # Move data to the GPU if available\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "        valid = valid.to(device)\n",
    "        fake = fake.to(device)\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate high-resolution images from low-resolution input\n",
    "        generated_hr_imgs = generator(lr_imgs)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        g_loss = adversarial_loss(discriminator(generated_hr_imgs), valid)\n",
    "        \n",
    "        # Content loss\n",
    "        g_loss += 0.1 * content_loss(generated_hr_imgs, hr_imgs)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real samples\n",
    "        real_loss = adversarial_loss(discriminator(hr_imgs), valid)\n",
    "        \n",
    "        # Measure discriminator's ability to classify fake samples\n",
    "        fake_loss = adversarial_loss(discriminator(generated_hr_imgs.detach()), fake)\n",
    "        \n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                  f\"Generator Loss: {g_loss.item():.4f}, Discriminator Loss: {d_loss.item():.4f}\")\n",
    "        \n",
    "    # Save some generated images\n",
    "    # if epoch % 5 == 0:\n",
    "    #     with torch.no_grad():\n",
    "    #         generated_imgs = generator(fixed_lr_imgs.to(device)).detach().cpu()\n",
    "    #         save_image(generated_imgs, f\"generated_images_{epoch}.png\", nrow=8, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "\n",
    "class noiseDataset(data.Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, lr_size=32):\n",
    "        super(noiseDataset, self).__init__()\n",
    "\n",
    "        base = dataset\n",
    "\n",
    "        self.noise_imgs = sorted(glob.glob(base + '*.png'))\n",
    "        self.pre_process = transforms.Compose([transforms.RandomCrop(size),\n",
    "                                               transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noise = self.pre_process(Image.open(self.noise_imgs[index]))\n",
    "        norm_noise = (noise - torch.mean(noise, dim=[1, 2], keepdim=True))\n",
    "        return norm_noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noise_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1736) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [168]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Example usage in training loop\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (hr_images, lr_images) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Here you would feed hr_images and lr_images into your GAN model for training\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Remember that hr_images are high-resolution and lr_images are low-resolution\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Training code for GAN goes here\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\saivi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1736) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_images = os.listdir(hr_dir)\n",
    "        self.lr_images = os.listdir(lr_dir)\n",
    "        self.transform_lr = transforms.Compose([\n",
    "                                transforms.Resize(32),  # Resize to a fixed size\n",
    "                                transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "                            ])\n",
    "        self.transform_hr = transforms.Compose([\n",
    "                                transforms.Resize(128),  # Resize to a fixed size\n",
    "                                transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "                            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.hr_images), len(self.lr_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_img_name = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        lr_img_name = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        \n",
    "        hr_image = Image.open(hr_img_name).convert(\"RGB\")\n",
    "        lr_image = Image.open(lr_img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            hr_image = self.transform(hr_image)\n",
    "            lr_image = self.transform(lr_image)\n",
    "\n",
    "        return hr_image, lr_image\n",
    "\n",
    "# Define transforms for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to a fixed size\n",
    "    transforms.ToTensor(),           # Convert PIL image to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the tensor\n",
    "])\n",
    "\n",
    "# Path to your HR and LR image folders\n",
    "hr_folder = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K copy\\valid\\HR\"\n",
    "lr_folder = r\"C:\\SaiVinay\\Thesis\\Code\\DataSets\\DF2K copy\\valid\\LR\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImageDataset(hr_folder, lr_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16,  num_workers=1)\n",
    "\n",
    "# Example usage in training loop\n",
    "for batch_idx, (hr_images, lr_images) in enumerate(dataloader):\n",
    "    # Here you would feed hr_images and lr_images into your GAN model for training\n",
    "    # Remember that hr_images are high-resolution and lr_images are low-resolution\n",
    "    # Training code for GAN goes here\n",
    "    pass  # Placeholder for actual training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define Generator and Discriminator architectures\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define layers for upsampling the LR image to SR image\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define layers for discriminating SR images\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define loss functions\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (lr_images, _) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(lr_images.size(0), 1, requires_grad=False)\n",
    "        fake = torch.zeros(lr_images.size(0), 1, requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        lr_images = lr_images.cuda()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a high-resolution image from the low-resolution input\n",
    "        sr_images = generator(lr_images)\n",
    "\n",
    "        # Adversarial loss\n",
    "        g_loss = criterion(discriminator(sr_images), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(lr_images), valid)\n",
    "        fake_loss = criterion(discriminator(sr_images.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, num_epochs, batch_idx, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "    # Save generated images\n",
    "    if epoch % 5 == 0:\n",
    "        save_image(sr_images.data[:25], \"images/sr_%d.png\" % epoch, nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
